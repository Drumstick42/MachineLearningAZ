{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drumstick42/MachineLearningAZ/blob/main/DataPreprocessing/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfu95BpfXNrz"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7P7sxtCYJ1m"
      },
      "source": [
        "#Pandas creates a data frame, kinda like R. \n",
        "dataset = pd.read_csv('Data.csv')\n",
        "\n",
        "# In any ML dataset, you have the features, and the dependent variable vector.\n",
        "# Features are the data with which we are trying to do the prediction (independent variable)\n",
        "# We want features in the first columns, and the dependent variable in the last column\n",
        "\n",
        "# In Data.csv, features are in the first 3 columns\n",
        "# NOTE: Ranges in python are exclusive @ upper bound\n",
        "features = dataset.iloc[:, :-1].values #iloc locates the rows/columns specified by the input. : gets entire range, :-1 is the range from the first element to the last index.\n",
        "dependent = dataset.iloc[:, -1].values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UErxQ4J1bXqS",
        "outputId": "a39dff95-de77-4f00-e9ab-ed2c65cc84ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(features)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mkc3l7obamh",
        "outputId": "825b539b-82dc-483e-b6bd-5f013b3041e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(dependent)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fswwBHFkcl2m"
      },
      "source": [
        "Options for missing data:\n",
        "\n",
        "  1.) Ignore that observation (okay if not a lot of missing data)\n",
        "  \n",
        "  2.) Replace missing value by the average of the rest of the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q5xHX8ecz0-"
      },
      "source": [
        "# scykit learn has a simpleimputer that will reduce missing data by average\n",
        "from sklearn.impute import SimpleImputer\n",
        "# pandas fills in our missing features with np.nan\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean') #Creates an instace of SimpleImputer. \n",
        "imputer.fit(features[:, 1:3]) #Connects imputer to matrix\n",
        "features[:, 1:3] = imputer.transform(features[:, 1:3]) #Performs the replacement using the mean values calculated from the features of the matrix input to fit. The transform *could* act on a different matrix"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eamXlXuYfAU_",
        "outputId": "03f6d456-8ba5-4c50-be7a-134219e50f06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(features)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0bTN2W_gmBW"
      },
      "source": [
        "Could encode countries by a number. Problem with that is that there could be misinterpretation about what the number represents. A numerical encoding could imply that the order of the rows matters, when they clearly don't.\n",
        "The better way is to create a feature vector for each country. The vectors contain bools as that denote whether or not a particular customer is from that country. This is calle One-Hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Acd9gWpnDT0"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "columnTransformer = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')\n",
        "features = np.array(columnTransformer.fit_transform(features)) # unlike above, performs fit and transform in same step\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEEWBG52o6Wj",
        "outputId": "0742c670-99fa-4522-979b-c82507f6d333",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(features)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw_AwmUJpZHa"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelEncoder = LabelEncoder()\n",
        "dependent = labelEncoder.fit_transform(dependent)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkSBJudgsToa",
        "outputId": "795f4129-ec22-41bd-9bac-b7f54101ff1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sidebar: what if we wanted yes to be 0, no to be 1?\n",
        "# it looks like the labelEncoder will just count, starting from the first label it finds.\n",
        "labelEncoder2 = LabelEncoder()\n",
        "print(labelEncoder2.fit_transform([\"Yes\", \"No\"])) # yes is still 1. why?\n",
        "# Looks like the encoding might act alphabetically."
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk2jaRKJsGyn",
        "outputId": "b799f639-514a-4968-80aa-6391547a78ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(dependent)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CaMAjLpuGst"
      },
      "source": [
        "Common question is whether feature scaling is done before or after splitting. The correct answer is after.\n",
        "Training set - train model on existing observations\n",
        "Test set - test model on future observations (we obviously already have them, but we treat it like future data.) \n",
        "\n",
        "This is why we do feature scaling after the data sets are split. We need to treat the test set as if it is future data. If we apply scaling before the split, then the test-set will change the scaling. \"Information leakage\" onto test-set. It will contain information that it shouldn't.\n",
        "\n",
        "80/20 is a good ratio for the training/test split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtNcGXqmvRC3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xTr, xTst, yTr, yTst = train_test_split(features, dependent, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sep8UaUI60Ls",
        "outputId": "bd7710da-0122-4551-d924-94e24656d260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(xTr)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hxsIyZP60TF",
        "outputId": "598c7073-6ed5-46a8-974a-189e08a8980d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(xTst)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhR073r760Zl",
        "outputId": "bbceef48-77a8-4332-a056-6e3cce11a23f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(yTr)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S33TZ_r360h1",
        "outputId": "125a270e-ecc3-4549-c645-62e917625066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(yTst)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wd1ebvtuT0V"
      },
      "source": [
        "Scale features to make sure that no feature dominates another, and all features are taken into account in the alogrithm. Not always needed (for example, multiple regression has constants for each feature that implicitly scale).\n",
        "\n",
        "Standardization: x_stand = (x - mean(x))/stddev(x), all features between ~-3 and ~3\n",
        "\n",
        "Normalization: (x - min(x))/(max(x) - min(x)), all features between 0 and 1\n",
        "\n",
        "Normalization reccommended when there's a normal distribution of features. Standardization works all the time, so we're just going to use that. \n",
        "\n",
        "NOTE: WE USE THE MEAN AND STDDEV FROM THE TRAINING SET TO SCALE THE TEST SET."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIXsY9E49uW_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Do not need to apply standardization to dummy variables (the columns created from the country labels). Already take values between -3 and +3.\n",
        "xTr[:, 3:] = scaler.fit_transform(xTr[:, 3:])\n",
        "xTst[:, 3:] = scaler.transform(xTst[:, 3:])\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh03cbON_wf2",
        "outputId": "21493f01-19d0-4f1f-db05-b103d13182eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(xTr)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]\n",
            " [0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]\n",
            " [1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]\n",
            " [0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]\n",
            " [0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]\n",
            " [1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]\n",
            " [0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]\n",
            " [1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzEZslFh_wQ8",
        "outputId": "0329331c-085a-4a72-9d76-0a58cfcb31bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(xTst)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830127 -0.9069571034860731]\n",
            " [1.0 0.0 0.0 -0.44973664397484425 0.20564033932253029]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}